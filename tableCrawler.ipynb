{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google # https://pypi.org/project/google/#files\n",
    "from googlesearch import search \n",
    "import pandas as pd\n",
    "import tabula\n",
    "import regex as re\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "import validators\n",
    "import urllib.parse\n",
    "import time\n",
    "import warnings\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "sys.setrecursionlimit(1000)\n",
    "# More on the google module here:\n",
    "#https://python-googlesearch.readthedocs.io/en/latest/index.html#module-googlesearch\n",
    "\n",
    "# An alternative here maybe:\n",
    "# https://github.com/Nv7-GitHub/googlesearch/blob/master/googlesearch/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get links hosting each LAs account statments.\n",
    "def getLinks(names):\n",
    "    \n",
    "    # Empty list for saving links.\n",
    "    links = []\n",
    "    \n",
    "    # Initialize search number (for pausing after a number of searches)\n",
    "    searchNum = 0\n",
    "    \n",
    "    for council in names:\n",
    "        print(council)\n",
    "        \n",
    "        # Increment search number by 1\n",
    "        searchNum += 1\n",
    "        print(searchNum)\n",
    "        \n",
    "        # Search terms (on query/council)\n",
    "        query = r'{} statement of accounts'.format(council)\n",
    "        \n",
    "        # Google search using 'query' above and save as object.\n",
    "        queryObj = search(query, tld = 'com', stop = 10)\n",
    "        \n",
    "        # Quick and dirty way of getting something from a generator object.\n",
    "        for j in queryObj:\n",
    "            if not j.endswith('pdf'):\n",
    "                links.append(j)\n",
    "                break\n",
    "        \n",
    "        # Add a pause to the search every 30th term to avoid 'Too Many Requests'\n",
    "        if searchNum%30 == 0:\n",
    "            time.sleep(1000)\n",
    "        \n",
    "        \n",
    "    return links    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking a single query\n",
    "\n",
    "# query = r'{} statement of accounts \"gov.uk\"'.format('bedford')\n",
    "\n",
    "# # Google search using 'query' above and save as object.\n",
    "# queryObj = search(query, tld = 'com', stop = 10)\n",
    "\n",
    "# # Quick and dirty way of getting something from a generator object.\n",
    "# for j in queryObj:\n",
    "#     if not j.endswith('pdf'):\n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get list of all councils\n",
    "councilNames = pd.read_csv('./councilNames.csv', sep = \",\")\n",
    "councilNames = councilNames['x'].str.lower()\n",
    "\n",
    "\n",
    "# Get link to page where account statements will be for each council.\n",
    "councilLinks = getLinks(councilNames)\n",
    "\n",
    "# Save as df -> csv \n",
    "dfLinks = pd.DataFrame(data = {\"Councils\": councilNames, \"Link\": councilLinks})\n",
    "dfLinks.to_csv(\"./councilLinks.csv\", sep = ',', index = False)\n",
    "\n",
    "# If the above already executed:\n",
    "# # Load csv\n",
    "# councilLinks_df = pd.read_csv('./councilLinks.csv')\n",
    "# # councilLinks = councilLinks_df['Link'] \n",
    "# councilLinks_df.set_index('Councils', inplace = True)\n",
    "\n",
    "# Check if they have the same length.\n",
    "print(councilLinks_df.shape[0])\n",
    "print(len(councilNames))\n",
    "\n",
    "# They have the same length, but we don't know \n",
    "# if we actually got one link per LA \n",
    "# e.g. some searches might've returned news articles or\n",
    "# other stuff.\n",
    "\n",
    "# It's not straighforward to check, so will go ahead\n",
    "# with the assumption that we got all relevant links\n",
    "# and see if there's any anomalies when getting the \n",
    "# PDFs from the pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLink(pdfs, councilLink, pdfLink):\n",
    "    \n",
    "    \n",
    "    # Join 'base' link of council with link for statement.\n",
    "    fullLink = urllib.parse.urljoin(councilLink, pdfLink['href'])\n",
    "    \n",
    "    # Add link to pdfs dict.\n",
    "    # 'Description': 'link'\n",
    "    pdfs[pdfLink.text] = fullLink\n",
    "    \n",
    "    return pdfs\n",
    "    \n",
    "\n",
    "def makeSearchTerms(yearL, yearH, other = []):\n",
    "    \n",
    "    # Different combinations of years\n",
    "    # Would be nice to get a nice regex here.\n",
    "    substrings_v1 = [yearL, yearH]\n",
    "    substrings_v2 = [yearL, yearH[2:]]\n",
    "    substrings_v3 = [yearL[2:], yearH[2:]]\n",
    "    substrings_v4 = [yearL + yearH]\n",
    "    substrings_v5 = [yearL + yearH[2:]]\n",
    "    \n",
    "    allSubstrings = [substrings_v1, substrings_v2, substrings_v3, substrings_v4, substrings_v5]\n",
    "    \n",
    "    if other != []:\n",
    "        for ss in allSubstrings:\n",
    "            ss.extend(other)\n",
    "    \n",
    "    return allSubstrings\n",
    "    \n",
    "\n",
    "\n",
    "def accessLink(councilLink):\n",
    "    try:\n",
    "        response = requests.get(councilLink, \n",
    "                                timeout = 15,\n",
    "                               headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.text, parse_only=SoupStrainer('a', href = True))\n",
    "        return soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def conditions(subStrVers, link):\n",
    "    '''\n",
    "    Checks if all elements of a list (of strings) are present in a link.\n",
    "    subStrVers is a list of lists, so that different versions of the lists can be checked.\n",
    "    Makes more sense after looking at findStatements() below.\n",
    "    \n",
    "    Input: \n",
    "        subStrVers - a list of lists.\n",
    "        link - a string. \n",
    "    \n",
    "    Outputs True if conditions are satisfied, False otherwise.\n",
    "    '''\n",
    "    satisfied = False\n",
    "    \n",
    "    # Search in file name/descriptions.\n",
    "    for ss in subStrVers:\n",
    "        if all(s in link.text.lower() for s in ss):\n",
    "            satisfied = True\n",
    "            break\n",
    "            \n",
    "    \n",
    "    # Search in link name.\n",
    "    for ss in subStrVers:\n",
    "        if all(s in link['href'].lower() for s in ss):\n",
    "            satisfied = True\n",
    "            break\n",
    "    \n",
    "\n",
    "    return satisfied        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# List all pdfs from a url.\n",
    "def findStatements(councilLink, yearL, yearH, otherTerms = [], pdfs = {}):  \n",
    "      \n",
    "    soup = accessLink(councilLink)\n",
    "    \n",
    "    # If site can't be accessed stop and return empty pdfs.\n",
    "    if soup == 0:\n",
    "        return pdfs\n",
    "\n",
    "    \n",
    "    allSubstrings = makeSearchTerms(yearL, yearH, other = otherTerms)\n",
    "    \n",
    "    # Find all files containing 'pdf'\n",
    "    for link in soup.select(\"a[href*='pdf']\"):\n",
    "        \n",
    "        if conditions(allSubstrings, link):\n",
    "            addLink(pdfs, councilLink, link)\n",
    "        \n",
    "\n",
    "    # If none found above, look at all links and see if terms\n",
    "    # can be found in the link or description\n",
    "    if pdfs == {}:\n",
    "        for link in soup.select(\"a\"):\n",
    "            if conditions(allSubstrings, link):\n",
    "                \n",
    "                # Add link to 'pdfs' dict. \n",
    "                addLink(pdfs, councilLink, link)\n",
    "                \n",
    "\n",
    "            \n",
    "    # Some councils have statements for different years\n",
    "    # on different webpages. If this is the case, figure out\n",
    "    # the relevant page, and call findStatements recurseively. \n",
    "    if pdfs == {}:\n",
    "        # This is really messy as I was basically trying \n",
    "        # to add conditions as I went, could probably use some cleaning up.\n",
    "        \n",
    "        # Initial candidate for new council link (i.e. selection)\n",
    "        selection = soup.select(\"a[href*='{}-{}']\".format(yearL,yearH))\n",
    "        \n",
    "        # If a link containing 'yearL'-'yearH' not available.\n",
    "        if len(selection) == 0:\n",
    "            # Try 'yearL' - 'yearH'[2:], e.g. '2017' - '18'\n",
    "            selection = soup.select(\"a[href*='{}-{}']\".format(yearL,yearH[2:]))\n",
    "            \n",
    "        # If 'yearL' - 'yearH'[2:] not available, try 'yearL'[2:] - 'yearH'[2:]\n",
    "        if len(selection) == 0:\n",
    "            selection = soup.select(\"a[href*='{}-{}']\".format(yearL[2:],yearH[2:]))\n",
    "                    \n",
    "        # If still haven't found anything, try the above w/ different format.\n",
    "        # e.g. '20172018' (instead of '2017'-'2018')\n",
    "        if len(selection) == 0:\n",
    "            selection = soup.select(\"a[href*='{}{}']\".format(yearL,yearH))  \n",
    "            \n",
    "        if len(selection) == 0:\n",
    "            selection = soup.select(\"a[href*='{}{}']\".format(yearL,yearH[2:]))  \n",
    "            \n",
    "        if len(selection) == 0:\n",
    "            selection = soup.select(\"a[href*='{}{}']\".format(yearL[2:],yearH[2:]))     \n",
    "            \n",
    "        # If none are available look for 'historical' in link name.       \n",
    "        if len(selection) == 0:\n",
    "            selection = soup.select(\"a[href*='historical']\")                  \n",
    "\n",
    "        # W/ the final selection, clean up the link and call finalStatements again\n",
    "        # with the new link.\n",
    "        for link in selection:\n",
    "#             print(\"New council link (relavant year)\", link['href'])\n",
    "\n",
    "            newLink = urllib.parse.urljoin(councilLink, link['href'])\n",
    "            print(\"Old link:\", councilLink)\n",
    "            print(\"New line:\", newLink)\n",
    "    \n",
    "            if newLink == councilLink:\n",
    "                break\n",
    "            else:    \n",
    "                findStatements(newLink, yearL, yearH, otherTerms, pdfs)\n",
    "\n",
    "    # Some of the accounts aren't actually pdf files\n",
    "    # So looking at the description of the file as opposed to\n",
    "    # the actual file extention for the term 'pdf'. \n",
    "    if pdfs == {}:\n",
    "        for link in soup.find_all('a', string = re.compile(\"pdf\", re.IGNORECASE)):\n",
    "            \n",
    "            # Add link to 'pdfs' dict.\n",
    "            addLink(pdfs, councilLink, link)\n",
    "            break\n",
    "    \n",
    "    # If all else fails, just get all the pdfs\n",
    "    if pdfs == {}:\n",
    "        for link in soup.select(\"a[href*='.pdf']\"):\n",
    "            \n",
    "            # Add link to pdfs dict.    \n",
    "            addLink(pdfs, councilLink, link)\n",
    "\n",
    "    # If that also fails get all asp.        \n",
    "    if pdfs == {}:\n",
    "        for link in soup.select(\"a[href*='asp']\"):\n",
    "\n",
    "            # Add link to pdfs dict.    \n",
    "            addLink(pdfs, councilLink, link)\n",
    "        \n",
    "            \n",
    "    return pdfs \n",
    "\n",
    "\n",
    "def getStatements(yearL, yearH):\n",
    "\n",
    "    allStatements = {el:{} for el in councilLinks_df.index.values.tolist()}\n",
    "    noPDFs = []\n",
    "    print(yearL, yearH)\n",
    "    for el in allStatements:\n",
    "\n",
    "        allStatements[el] = findStatements(councilLink = councilLinks_df.loc[el]['Link'], \\\n",
    "                                           yearL = yearL, \\\n",
    "                                           yearH = yearH, \\\n",
    "#                                            otherTerms = ['accounts'],\\\n",
    "                                           pdfs = {})\n",
    "        \n",
    "#         allStatements[el] = checkStatements(allStatements[el])\n",
    "        print(\"Got council: {} ({} links found)\".format(el, len(allStatements[el])))\n",
    "\n",
    "        if len(allStatements[el]) == 0:\n",
    "            noPDFs.append(el)\n",
    "            \n",
    "            \n",
    "    return allStatements, noPDFs        \n",
    "\n",
    "\n",
    "def conditionsDict(subStrVers, key):\n",
    "    \n",
    "    satisfied = False\n",
    "    \n",
    "    for ss in subStrVers:\n",
    "        if all(s in key for s in ss):\n",
    "            satisfied = True\n",
    "            \n",
    "    return satisfied   \n",
    "\n",
    "\n",
    "\n",
    "def checkStatement(pdfs, yearL, yearH, otherTerms = []):\n",
    "    \n",
    "                   \n",
    "    testTerms = makeSearchTerms(yearL, yearH, other = otherTerms)\n",
    "    \n",
    "\n",
    "    res = [term for key, \n",
    "           term in pdfs.items() \n",
    "           if (conditionsDict(testTerms, key) or\n",
    "          conditionsDict(testTerms, term))]\n",
    "    \n",
    "    if res != []:\n",
    "        return res[0]\n",
    "\n",
    "    \n",
    "    return res\n",
    "  \n",
    "\n",
    "def checkAllStatements(allStatements, yearL, yearH, otherTerms = []):\n",
    "\n",
    "    for council in allStatements:\n",
    "        \n",
    "        allStatements[council] = checkStatement(allStatements[council],\n",
    "                                               yearL, yearH,\n",
    "                                               otherTerms = otherTerms)\n",
    "            \n",
    "    return allStatements        \n",
    "\n",
    "\n",
    "def countLinks(allStatements):\n",
    "    \n",
    "    count_zero = 0\n",
    "    count_more = 0\n",
    "\n",
    "    for el in allStatements:\n",
    "        if len(allStatements[el]) == 0:\n",
    "#             print(\"Zero:\", el)\n",
    "            count_zero +=1\n",
    "\n",
    "        if len(allStatements[el]) > 1:\n",
    "#             print(\"More than 1: \", el)\n",
    "            count_more +=1\n",
    "\n",
    "    print(\"\\tLAs with 0 links:\", count_zero)\n",
    "#     print(\"\\tLAs with more than 1 link:\", count_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get statements from 2017 - 2018\n",
    "st_17_18, missing_17_18 = getStatements('2017', '2018')\n",
    "\n",
    "# Checking statements and getting single links.\n",
    "# st_17_18_checked = copy.deepcopy(st_17_18)\n",
    "# st_17_18_checked = checkAllStatements(st_17_18_checked, '2017', '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get statements from 2018 - 2019\n",
    "st_18_19, missing_18_19 = getStatements('2018', '2019')\n",
    "\n",
    "# Run some checks\n",
    "st_18_19_checked = copy.deepcopy(st_18_19)\n",
    "st_18_19_checked = checkAllStatements(st_18_19_checked, '2018', '2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statements from 2019 - 2020\n",
    "st_19_20, missing_19_20 = getStatements('2019', '2020')\n",
    "\n",
    "# Run some checks\n",
    "st_19_20_checked = copy.deepcopy(st_19_20)\n",
    "st_19_20_checked = checkAllStatements(st_19_20_checked, '2019', '2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCounts before checks 2017 - 2018 (# LA):\")\n",
    "countLinks(st_17_18)\n",
    "\n",
    "print(\"\\nCounts after checks 2017 - 2018 (# LA):\")\n",
    "countLinks(st_17_18_checked)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"\\nCounts before checks 2018 - 2019 (# LA):\")\n",
    "countLinks(st_18_19)\n",
    "\n",
    "print(\"\\nCounts after checks 2018 - 2019 (# LA):\")\n",
    "countLinks(st_18_19_checked)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"\\nCounts before checks 2019 - 2020 (# LA):\")\n",
    "countLinks(st_19_20)\n",
    "\n",
    "print(\"\\nCounts after checks 2019 - 2020 (# LA):\")\n",
    "countLinks(st_19_20_checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_statements(statements, filename):\n",
    "    filename = filename\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(json.dumps(statements))\n",
    "        \n",
    "def load_statements(filename):\n",
    "    with open(filename) as f:\n",
    "        statements = json.loads(f.read())\n",
    "    return statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the statements (json-like documents)\n",
    "save_statements(st_17_18_checked, 'st_17_18')\n",
    "save_statements(st_18_19_checked, 'st_18_19')\n",
    "save_statements(st_19_20_checked, 'st_19_20')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
